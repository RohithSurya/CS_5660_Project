{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"gnYNjlnxxGgJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-20 02:29:26.939863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     /home/codespace/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/codespace/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import os \n","import shutil\n","import random as rnd\n","\n","# import relevant libraries\n","import trax\n","import trax.fastmath.numpy as np\n","from trax import layers as tl\n","from trax import fastmath\n","# import Layer from the utils.py file\n","from utils import Layer, load_tweets, process_tweet\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684127501810,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"nzt0pQ0tyg1Z","outputId":"2558d5b3-7c3f-49ac-8bbf-ce639002decf"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>is_sarcastic</th>\n","      <th>headline</th>\n","      <th>article_link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>thirtysomething scientists unveil doomsday clo...</td>\n","      <td>https://www.theonion.com/thirtysomething-scien...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>dem rep. totally nails why congress is falling...</td>\n","      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>eat your veggies: 9 deliciously different recipes</td>\n","      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>inclement weather prevents liar from getting t...</td>\n","      <td>https://local.theonion.com/inclement-weather-p...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>mother comes pretty close to using word 'strea...</td>\n","      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   is_sarcastic                                           headline  \\\n","0             1  thirtysomething scientists unveil doomsday clo...   \n","1             0  dem rep. totally nails why congress is falling...   \n","2             0  eat your veggies: 9 deliciously different recipes   \n","3             1  inclement weather prevents liar from getting t...   \n","4             1  mother comes pretty close to using word 'strea...   \n","\n","                                        article_link  \n","0  https://www.theonion.com/thirtysomething-scien...  \n","1  https://www.huffingtonpost.com/entry/donna-edw...  \n","2  https://www.huffingtonpost.com/entry/eat-your-...  \n","3  https://local.theonion.com/inclement-weather-p...  \n","4  https://www.theonion.com/mother-comes-pretty-c...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n","df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-nN1oSzHzL8Z"},"outputs":[],"source":["del df['article_link']"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684127506147,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"k_QISxobzczD","outputId":"f6cb57e0-1357-4e6e-ecc1-27eabb9d20c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["13634\n","14985\n"]}],"source":["sarcastic_pd = df.loc[df['is_sarcastic']==1]\n","un_sarcastic_pd = df.loc[df['is_sarcastic']==0]\n","print(len(sarcastic_pd))\n","print(len(un_sarcastic_pd))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"bTBv-ZER0Ns1"},"outputs":[],"source":["sarcastic = list(sarcastic_pd['headline'])\n","un_sarcastic = list(un_sarcastic_pd['headline'])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"cPCHI8y70wcY"},"outputs":[],"source":["def train_test_split(sarcastic, un_sarcastic):\n","\n","  print(f\"The number of positive tweets: {len(sarcastic)}\")\n","  print(f\"The number of negative tweets: {len(un_sarcastic)}\")\n","  val_pos = sarcastic[10907:]\n","  train_pos = sarcastic[:10907]\n","\n","  val_neg = un_sarcastic[11988:]\n","  train_neg = un_sarcastic[:11988]\n","  \n","  train_x = train_pos + train_neg\n","  val_x = val_pos + val_neg\n","\n","  train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","  val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n","\n","  return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684127971082,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"G4sgR7Sg17Mp","outputId":"e3408950-3da0-44f1-9d6e-d0e773ab27b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]},{"name":"stdout","output_type":"stream","text":["The number of positive tweets: 13634\n","The number of negative tweets: 14985\n","length of train_x 22895\n","length of val_x 5724\n"]}],"source":["train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_test_split(sarcastic, un_sarcastic)\n","print(f\"length of train_x {len(train_x)}\")\n","print(f\"length of val_x {len(val_x)}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8260,"status":"ok","timestamp":1684128124517,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"Bfwabolx2uq6","outputId":"3d3e8735-efc0-4463-974c-c0c959aa48b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words in vocab are 19337\n"]}],"source":["def get_vocab(train_x):\n","\n","    # Include special tokens \n","    # started with pad, end of line and unk tokens\n","    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n","\n","    # Note that we build vocab using training data\n","    for tweet in train_x: \n","        processed_tweet = process_tweet(tweet)\n","        for word in processed_tweet:\n","            if word not in Vocab: \n","                Vocab[word] = len(Vocab)\n","    \n","    return Vocab\n","\n","Vocab = get_vocab(train_x)\n","\n","print(\"Total words in vocab are\",len(Vocab))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Zy7m8uXc27O-"},"outputs":[],"source":["def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n","    '''\n","    Input: \n","        tweet - A string containing a tweet\n","        vocab_dict - The words dictionary\n","        unk_token - The special string for unknown tokens\n","        verbose - Print info durign runtime\n","    Output:\n","        tensor_l - A python list with\n","        \n","    '''     \n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","    # Process the tweet into a list of words\n","    # where only important words are kept (stop words removed)\n","    word_l = process_tweet(tweet)\n","    \n","    if verbose:\n","        print(\"List of words from the processed tweet:\")\n","        print(word_l)\n","        \n","    # Initialize the list that will contain the unique integer IDs of each word\n","    tensor_l = [] \n","    \n","    # Get the unique integer ID of the __UNK__ token\n","    unk_ID = vocab_dict.get(unk_token)\n","    \n","    if verbose:\n","        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n","        \n","    # for each word in the list:\n","    for word in word_l:\n","        \n","        # Get the unique integer ID.\n","        # If the word doesn't exist in the vocab dictionary,\n","        # use the unique ID for __UNK__ instead.        \n","        word_ID = vocab_dict.get(word, unk_ID)\n","            \n","        # Append the unique integer ID to the tensor list.\n","        tensor_l.append(word_ID)\n","    ### END CODE HERE ###\n","    \n","    return tensor_l"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684128221819,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"x-HYKRtq3TGD","outputId":"fbfb262f-ef14-483b-f450-a75d8c3f0164"},"outputs":[{"name":"stdout","output_type":"stream","text":["Actual tweet is\n"," local muppet held for questioning in chicken sex ring\n","\n","Tensor of tweet:\n"," [291, 2, 2557, 2590, 1114, 1588, 1092]\n"]}],"source":["print(\"Actual tweet is\\n\", val_pos[0])\n","print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Yd176RqQ3V0u"},"outputs":[],"source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED: Data generator\n","def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n","    '''\n","    Input: \n","        data_pos - Set of positive examples\n","        data_neg - Set of negative examples\n","        batch_size - number of samples per batch. Must be even\n","        loop - True or False\n","        vocab_dict - The words dictionary\n","        shuffle - Shuffle the data order\n","    Yield:\n","        inputs - Subset of positive and negative examples\n","        targets - The corresponding labels for the subset\n","        example_weights - An array specifying the importance of each example\n","        \n","    '''     \n","\n","    # make sure the batch size is an even number\n","    # to allow an equal number of positive and negative samples    \n","    assert batch_size % 2 == 0\n","    \n","    # Number of positive examples in each batch is half of the batch size\n","    # same with number of negative examples in each batch\n","    n_to_take = batch_size // 2\n","    \n","    # Use pos_index to walk through the data_pos array\n","    # same with neg_index and data_neg\n","    pos_index = 0\n","    neg_index = 0\n","    \n","    len_data_pos = len(data_pos)\n","    len_data_neg = len(data_neg)\n","    \n","    # Get and array with the data indexes\n","    pos_index_lines = list(range(len_data_pos))\n","    neg_index_lines = list(range(len_data_neg))\n","    \n","    # shuffle lines if shuffle is set to True\n","    if shuffle:\n","        rnd.shuffle(pos_index_lines)\n","        rnd.shuffle(neg_index_lines)\n","        \n","    stop = False\n","    \n","    # Loop indefinitely\n","    while not stop:  \n","        \n","        # create a batch with positive and negative examples\n","        batch = []\n","        \n","        # First part: Pack n_to_take positive examples\n","        \n","        # Start from 0 and increment i up to n_to_take\n","        for i in range(n_to_take):\n","                    \n","            # If the positive index goes past the positive dataset,\n","            if pos_index >= len_data_pos: \n","                \n","                # If loop is set to False, break once we reach the end of the dataset\n","                if not loop:\n","                    stop = True;\n","                    break;\n","                # If user wants to keep re-using the data, reset the index\n","                pos_index = 0\n","                if shuffle:\n","                    # Shuffle the index of the positive sample\n","                    rnd.shuffle(pos_index_lines)\n","                    \n","            # get the tweet as pos_index\n","            tweet = data_pos[pos_index_lines[pos_index]]\n","            \n","            # convert the tweet into tensors of integers representing the processed words\n","            tensor = tweet_to_tensor(tweet, vocab_dict)\n","            \n","            # append the tensor to the batch list\n","            batch.append(tensor)\n","            \n","            # Increment pos_index by one\n","            pos_index = pos_index + 1\n","\n","\n","            \n","        ### START CODE HERE (Replace instances of 'None' with your code) ###\n","\n","        # Second part: Pack n_to_take negative examples\n","\n","        # Using the same batch list, start from 0 and increment i up to n_to_take\n","        for i in range(n_to_take):\n","            \n","            # If the negative index goes past the negative dataset,\n","            if neg_index >= len_data_neg:\n","                \n","                # If loop is set to False, break once we reach the end of the dataset\n","                if not loop:\n","                    stop = True \n","                    break \n","                    \n","                # If user wants to keep re-using the data, reset the index\n","                neg_index = 0\n","                \n","                if shuffle:\n","                    # Shuffle the index of the negative sample\n","                    rnd.shuffle(neg_index_lines)\n","                    \n","            # get the tweet as neg_index\n","            tweet = data_neg[neg_index_lines[neg_index]]\n","            \n","            # convert the tweet into tensors of integers representing the processed words\n","            tensor = tweet_to_tensor(tweet, vocab_dict)\n","            \n","            # append the tensor to the batch list\n","            batch.append(tensor)\n","            \n","            # Increment neg_index by one\n","            neg_index = neg_index + 1\n","\n","        ### END CODE HERE ###        \n","\n","        if stop:\n","            break;\n","\n","        # Get the max tweet length (the length of the longest tweet) \n","        # (you will pad all shorter tweets to have this length)\n","        max_len = max([len(t) for t in batch]) \n","        \n","        \n","        # Initialize the input_l, which will \n","        # store the padded versions of the tensors\n","        tensor_pad_l = []\n","        # Pad shorter tweets with zeros\n","        for tensor in batch:\n","\n","\n","        ### START CODE HERE (Replace instances of 'None' with your code) ###\n","            # Get the number of positions to pad for this tensor so that it will be max_len long\n","            n_pad = max_len-len(tensor)\n","            \n","            # Generate a list of zeros, with length n_pad\n","            pad_l = [0]*n_pad\n","            \n","            # concatenate the tensor and the list of padded zeros\n","            tensor_pad = tensor + pad_l\n","            \n","            # append the padded tensor to the list of padded tensors\n","            tensor_pad_l.append(tensor_pad)\n","\n","        # convert the list of padded tensors to a numpy array\n","        # and store this as the model inputs\n","        inputs = np.array(tensor_pad_l)\n","  \n","        # Generate the list of targets for the positive examples (a list of ones)\n","        # The length is the number of positive examples in the batch\n","        target_pos = [1]*n_to_take\n","        \n","        # Generate the list of targets for the negative examples (a list of zeros)\n","        # The length is the number of negative examples in the batch\n","        target_neg = [0]*n_to_take\n","        \n","        # Concatenate the positve and negative targets\n","        target_l = target_pos + target_neg\n","        \n","        # Convert the target list into a numpy array\n","        targets = np.array(target_l)\n","\n","        # Example weights: Treat all examples equally importantly.\n","        example_weights = np.ones_like(targets)\n","        \n","\n","        ### END CODE HERE ###\n","\n","        # note we use yield and not return\n","        yield inputs, targets, example_weights"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1684128302894,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"YhTWBguc3ehu","outputId":"2cd8d0fa-0fb1-4ddd-8749-92ea09185323"},"outputs":[{"name":"stdout","output_type":"stream","text":["Inputs: [[  113  8545  8931  3887  1693   570  1722  1721  8932  8933]\n"," [   54   778   339  4716   839  2363   792   524  3867     0]\n"," [  420  2189  1862   795  1868  4573     0     0     0     0]\n"," [19006   564 11902  2882  4710     0     0     0     0     0]]\n","Targets: [1 1 0 0]\n","Example Weights: [1 1 1 1]\n"]}],"source":["# Set the random number generator for the shuffle procedure\n","rnd.seed(30) \n","\n","# Create the training data generator\n","\n","def train_generator(batch_size, train_pos\n","                    , train_neg, vocab_dict, loop=True\n","                    , shuffle = False):\n","    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n","\n","# Create the validation data generator\n","def val_generator(batch_size, val_pos\n","                    , val_neg, vocab_dict, loop=True\n","                    , shuffle = False):\n","    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n","\n","# Create the validation data generator\n","def test_generator(batch_size, val_pos\n","                    , val_neg, vocab_dict, loop=False\n","                    , shuffle = False):\n","    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n","\n","# Get a batch from the train_generator and inspect.\n","inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n","\n","# this will print a list of 4 tensors padded with zeros\n","print(f'Inputs: {inputs}')\n","print(f'Targets: {targets}')\n","print(f'Example Weights: {example_weights}')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2z6Aku673ovT"},"outputs":[],"source":["# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: classifier\n","def classifier(vocab_size, embedding_dim=256, output_dim=2, mode='train'):\n","    \n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","        \n","    # create embedding layer\n","    embed_layer = tl.Embedding( \n","        vocab_size=vocab_size, # Size of the vocabulary\n","        d_feature=embedding_dim # Embedding dimension\n","    ) \n","    \n","    # Create a mean layer, to create an \"average\" word embedding\n","    mean_layer = tl.Mean(axis=1)\n","    \n","    # Create a dense layer, one unit for each output\n","    dense_middle_layer = tl.Dense(n_units = 128)\n","\n","    dense_another_middle_layer = tl.Dense(n_units=64)\n","\n","    drop_out_layer = tl.Dropout(rate=0.1, mode='train')\n","\n","    dense_one_another_middle_layer = tl.Dense(n_units=32)\n","\n","\n","    dense_output_layer = tl.Dense(n_units = output_dim)\n","    \n","    # Create the log softmax layer (no parameters needed)\n","    log_softmax_layer = tl.LogSoftmax()\n","    \n","    # Use tl.Serial to combine all layers\n","    # and create the classifier\n","    # of type trax.layers.combinators.Serial\n","    model = tl.Serial( \n","      embed_layer, # embedding layer\n","      mean_layer, # mean layer\n","      dense_middle_layer,\n","      tl.Relu(),\n","      dense_another_middle_layer,\n","      tl.Relu(),\n","      drop_out_layer,\n","      dense_one_another_middle_layer,\n","      tl.Relu(),\n","      dense_output_layer, # dense output layer\n","      log_softmax_layer # log softmax layer\n","    ) \n","    ### END CODE HERE ###\n","    \n","    # return the model of type\n","    return model"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"oPLOf0xL33lU"},"outputs":[],"source":["tmp_model = classifier(vocab_size=len(Vocab))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684132405360,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"VHwlVqhm39dF","outputId":"0eb69be6-19d5-4093-e7ea-a95cae0ebe11"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'trax.layers.combinators.Serial'>\n"]},{"data":{"text/plain":["Serial[\n","  Embedding_19337_256\n","  Mean\n","  Dense_128\n","  Serial[\n","    Relu\n","  ]\n","  Dense_64\n","  Serial[\n","    Relu\n","  ]\n","  Dropout\n","  Dense_32\n","  Serial[\n","    Relu\n","  ]\n","  Dense_2\n","  LogSoftmax\n","]"]},"metadata":{},"output_type":"display_data"}],"source":["print(type(tmp_model))\n","display(tmp_model)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"1p07qfxj4Kny"},"outputs":[],"source":["# PLEASE, DO NOT MODIFY OR DELETE THIS CELL\n","from trax.supervised import training\n","\n","def get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, vocab_dict, loop, batch_size = 16):\n","    \n","    rnd.seed(271)\n","\n","    train_task = training.TrainTask(\n","        labeled_data=train_generator(batch_size, train_pos\n","                    , train_neg, vocab_dict, loop\n","                    , shuffle = True),\n","        loss_layer=tl.WeightedCategoryCrossEntropy(),\n","        optimizer=trax.optimizers.Adam(0.001),\n","        n_steps_per_checkpoint=10,\n","    )\n","\n","    eval_task = training.EvalTask(\n","        labeled_data=val_generator(batch_size, val_pos\n","                    , val_neg, vocab_dict, loop\n","                    , shuffle = True),        \n","        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n","    )\n","    \n","    return train_task, eval_task\n","    \n","\n","train_task, eval_task = get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, Vocab, True, batch_size = 16)\n","model = classifier(vocab_size=len(Vocab))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684132415969,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"koBQ5NJh4LvB","outputId":"a93649ea-c190-4e64-af3a-e64e842e11a1"},"outputs":[{"data":{"text/plain":["Serial[\n","  Embedding_19337_256\n","  Mean\n","  Dense_128\n","  Serial[\n","    Relu\n","  ]\n","  Dense_64\n","  Serial[\n","    Relu\n","  ]\n","  Dropout\n","  Dense_32\n","  Serial[\n","    Relu\n","  ]\n","  Dense_2\n","  LogSoftmax\n","]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"CKCt6VJF4tuU"},"outputs":[],"source":["# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: train_model\n","def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n","    '''\n","    Input: \n","        classifier - the model you are building\n","        train_task - Training task\n","        eval_task - Evaluation task. Received as a list.\n","        n_steps - the evaluation steps\n","        output_dir - folder to save your files\n","    Output:\n","        trainer -  trax trainer\n","    '''\n","    rnd.seed(31) # Do NOT modify this random seed. This makes the notebook easier to replicate\n","    \n","    ### START CODE HERE (Replace instances of 'None' with your code) ###          \n","    training_loop = training.Loop( \n","                                classifier, # The learning model\n","                                train_task, # The training task\n","                                eval_tasks=eval_task, # The evaluation task\n","                                output_dir=output_dir, # The output directory\n","                                random_seed=31 # Do not modify this random seed in order to ensure reproducibility and for grading purposes.\n","    ) \n","\n","    training_loop.run(n_steps = n_steps)\n","    ### END CODE HERE ###\n","    \n","    # Return the training_loop, since it has the model.\n","    return training_loop"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1684132421841,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"-WO5qUQu43ME","outputId":"7f3b48f9-550f-4026-d542-b274179f00bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["./model/\n"]}],"source":["dir_path = './model/'\n","\n","try:\n","    shutil.rmtree(dir_path)\n","except OSError as e:\n","    pass\n","\n","\n","output_dir = './model/'\n","output_dir_expand = os.path.expanduser(output_dir)\n","print(output_dir_expand)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154211,"status":"ok","timestamp":1684132577970,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"wvelB2UT4vZf","outputId":"a616abec-5a1a-4e73-c95c-232be1041367"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/codespace/.python/current/lib/python3.10/site-packages/jax/_src/xla_bridge.py:674: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n","  warnings.warn(\n","/home/codespace/.python/current/lib/python3.10/site-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n","  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"]},{"name":"stdout","output_type":"stream","text":["\n","Step      1: Total number of trainable weights: 4993570\n","Step      1: Ran 1 train steps in 2.26 secs\n","Step      1: train WeightedCategoryCrossEntropy |  0.69127071\n"]},{"name":"stderr","output_type":"stream","text":["/home/codespace/.python/current/lib/python3.10/site-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n","  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"]},{"name":"stdout","output_type":"stream","text":["Step      1: eval  WeightedCategoryCrossEntropy |  0.68969113\n","Step      1: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step     10: Ran 9 train steps in 3.93 secs\n","Step     10: train WeightedCategoryCrossEntropy |  0.69289327\n","Step     10: eval  WeightedCategoryCrossEntropy |  0.69142061\n","Step     10: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step     20: Ran 10 train steps in 4.19 secs\n","Step     20: train WeightedCategoryCrossEntropy |  0.68806088\n","Step     20: eval  WeightedCategoryCrossEntropy |  0.67185402\n","Step     20: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step     30: Ran 10 train steps in 2.60 secs\n","Step     30: train WeightedCategoryCrossEntropy |  0.67981511\n","Step     30: eval  WeightedCategoryCrossEntropy |  0.69828600\n","Step     30: eval      WeightedCategoryAccuracy |  0.50000000\n","\n","Step     40: Ran 10 train steps in 1.69 secs\n","Step     40: train WeightedCategoryCrossEntropy |  0.67616284\n","Step     40: eval  WeightedCategoryCrossEntropy |  0.69476938\n","Step     40: eval      WeightedCategoryAccuracy |  0.50000000\n","\n","Step     50: Ran 10 train steps in 2.48 secs\n","Step     50: train WeightedCategoryCrossEntropy |  0.67387116\n","Step     50: eval  WeightedCategoryCrossEntropy |  0.66921878\n","Step     50: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step     60: Ran 10 train steps in 1.64 secs\n","Step     60: train WeightedCategoryCrossEntropy |  0.68408817\n","Step     60: eval  WeightedCategoryCrossEntropy |  0.64841896\n","Step     60: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step     70: Ran 10 train steps in 1.70 secs\n","Step     70: train WeightedCategoryCrossEntropy |  0.66751838\n","Step     70: eval  WeightedCategoryCrossEntropy |  0.67175680\n","Step     70: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step     80: Ran 10 train steps in 1.72 secs\n","Step     80: train WeightedCategoryCrossEntropy |  0.65765774\n","Step     80: eval  WeightedCategoryCrossEntropy |  0.67003590\n","Step     80: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step     90: Ran 10 train steps in 1.68 secs\n","Step     90: train WeightedCategoryCrossEntropy |  0.64415896\n","Step     90: eval  WeightedCategoryCrossEntropy |  0.62730891\n","Step     90: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step    100: Ran 10 train steps in 1.75 secs\n","Step    100: train WeightedCategoryCrossEntropy |  0.64239037\n","Step    100: eval  WeightedCategoryCrossEntropy |  0.54453319\n","Step    100: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    110: Ran 10 train steps in 1.73 secs\n","Step    110: train WeightedCategoryCrossEntropy |  0.64291799\n","Step    110: eval  WeightedCategoryCrossEntropy |  0.56298059\n","Step    110: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    120: Ran 10 train steps in 1.74 secs\n","Step    120: train WeightedCategoryCrossEntropy |  0.61021852\n","Step    120: eval  WeightedCategoryCrossEntropy |  0.53235883\n","Step    120: eval      WeightedCategoryAccuracy |  0.87500000\n","\n","Step    130: Ran 10 train steps in 1.81 secs\n","Step    130: train WeightedCategoryCrossEntropy |  0.59769052\n","Step    130: eval  WeightedCategoryCrossEntropy |  0.89587361\n","Step    130: eval      WeightedCategoryAccuracy |  0.43750000\n","\n","Step    140: Ran 10 train steps in 1.77 secs\n","Step    140: train WeightedCategoryCrossEntropy |  0.62564749\n","Step    140: eval  WeightedCategoryCrossEntropy |  0.58230561\n","Step    140: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    150: Ran 10 train steps in 1.81 secs\n","Step    150: train WeightedCategoryCrossEntropy |  0.58970106\n","Step    150: eval  WeightedCategoryCrossEntropy |  0.65794796\n","Step    150: eval      WeightedCategoryAccuracy |  0.50000000\n","\n","Step    160: Ran 10 train steps in 1.86 secs\n","Step    160: train WeightedCategoryCrossEntropy |  0.54211140\n","Step    160: eval  WeightedCategoryCrossEntropy |  0.49878758\n","Step    160: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    170: Ran 10 train steps in 1.87 secs\n","Step    170: train WeightedCategoryCrossEntropy |  0.65720522\n","Step    170: eval  WeightedCategoryCrossEntropy |  0.50567853\n","Step    170: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    180: Ran 10 train steps in 1.87 secs\n","Step    180: train WeightedCategoryCrossEntropy |  0.58642048\n","Step    180: eval  WeightedCategoryCrossEntropy |  0.48576880\n","Step    180: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    190: Ran 10 train steps in 1.90 secs\n","Step    190: train WeightedCategoryCrossEntropy |  0.56598198\n","Step    190: eval  WeightedCategoryCrossEntropy |  0.52021790\n","Step    190: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    200: Ran 10 train steps in 1.90 secs\n","Step    200: train WeightedCategoryCrossEntropy |  0.55539268\n","Step    200: eval  WeightedCategoryCrossEntropy |  0.46093136\n","Step    200: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    210: Ran 10 train steps in 1.88 secs\n","Step    210: train WeightedCategoryCrossEntropy |  0.58070004\n","Step    210: eval  WeightedCategoryCrossEntropy |  0.65607101\n","Step    210: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    220: Ran 10 train steps in 1.91 secs\n","Step    220: train WeightedCategoryCrossEntropy |  0.53595954\n","Step    220: eval  WeightedCategoryCrossEntropy |  0.65192699\n","Step    220: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step    230: Ran 10 train steps in 2.06 secs\n","Step    230: train WeightedCategoryCrossEntropy |  0.49126562\n","Step    230: eval  WeightedCategoryCrossEntropy |  0.47050378\n","Step    230: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    240: Ran 10 train steps in 2.01 secs\n","Step    240: train WeightedCategoryCrossEntropy |  0.60879946\n","Step    240: eval  WeightedCategoryCrossEntropy |  0.64604372\n","Step    240: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step    250: Ran 10 train steps in 2.02 secs\n","Step    250: train WeightedCategoryCrossEntropy |  0.58949208\n","Step    250: eval  WeightedCategoryCrossEntropy |  0.52969390\n","Step    250: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    260: Ran 10 train steps in 2.08 secs\n","Step    260: train WeightedCategoryCrossEntropy |  0.50080234\n","Step    260: eval  WeightedCategoryCrossEntropy |  0.64511430\n","Step    260: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    270: Ran 10 train steps in 2.07 secs\n","Step    270: train WeightedCategoryCrossEntropy |  0.57241964\n","Step    270: eval  WeightedCategoryCrossEntropy |  0.41446838\n","Step    270: eval      WeightedCategoryAccuracy |  0.93750000\n","\n","Step    280: Ran 10 train steps in 2.06 secs\n","Step    280: train WeightedCategoryCrossEntropy |  0.49690136\n","Step    280: eval  WeightedCategoryCrossEntropy |  0.55368525\n","Step    280: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    290: Ran 10 train steps in 2.86 secs\n","Step    290: train WeightedCategoryCrossEntropy |  0.57307285\n","Step    290: eval  WeightedCategoryCrossEntropy |  0.64913279\n","Step    290: eval      WeightedCategoryAccuracy |  0.56250000\n","\n","Step    300: Ran 10 train steps in 2.09 secs\n","Step    300: train WeightedCategoryCrossEntropy |  0.54832125\n","Step    300: eval  WeightedCategoryCrossEntropy |  0.61575419\n","Step    300: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    310: Ran 10 train steps in 2.10 secs\n","Step    310: train WeightedCategoryCrossEntropy |  0.54468715\n","Step    310: eval  WeightedCategoryCrossEntropy |  0.50038207\n","Step    310: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    320: Ran 10 train steps in 2.13 secs\n","Step    320: train WeightedCategoryCrossEntropy |  0.52359623\n","Step    320: eval  WeightedCategoryCrossEntropy |  0.61615217\n","Step    320: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    330: Ran 10 train steps in 2.11 secs\n","Step    330: train WeightedCategoryCrossEntropy |  0.55779612\n","Step    330: eval  WeightedCategoryCrossEntropy |  0.57363760\n","Step    330: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    340: Ran 10 train steps in 2.11 secs\n","Step    340: train WeightedCategoryCrossEntropy |  0.49927416\n","Step    340: eval  WeightedCategoryCrossEntropy |  0.37150282\n","Step    340: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    350: Ran 10 train steps in 2.20 secs\n","Step    350: train WeightedCategoryCrossEntropy |  0.58195132\n","Step    350: eval  WeightedCategoryCrossEntropy |  0.54549861\n","Step    350: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    360: Ran 10 train steps in 2.13 secs\n","Step    360: train WeightedCategoryCrossEntropy |  0.54883945\n","Step    360: eval  WeightedCategoryCrossEntropy |  0.69893873\n","Step    360: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    370: Ran 10 train steps in 2.18 secs\n","Step    370: train WeightedCategoryCrossEntropy |  0.51400697\n","Step    370: eval  WeightedCategoryCrossEntropy |  0.41096929\n","Step    370: eval      WeightedCategoryAccuracy |  0.87500000\n","\n","Step    380: Ran 10 train steps in 2.21 secs\n","Step    380: train WeightedCategoryCrossEntropy |  0.52013928\n","Step    380: eval  WeightedCategoryCrossEntropy |  0.69830561\n","Step    380: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    390: Ran 10 train steps in 2.16 secs\n","Step    390: train WeightedCategoryCrossEntropy |  0.45550495\n","Step    390: eval  WeightedCategoryCrossEntropy |  0.45873496\n","Step    390: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    400: Ran 10 train steps in 2.19 secs\n","Step    400: train WeightedCategoryCrossEntropy |  0.52862501\n","Step    400: eval  WeightedCategoryCrossEntropy |  0.50341356\n","Step    400: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    410: Ran 10 train steps in 2.22 secs\n","Step    410: train WeightedCategoryCrossEntropy |  0.50472450\n","Step    410: eval  WeightedCategoryCrossEntropy |  0.34699160\n","Step    410: eval      WeightedCategoryAccuracy |  0.87500000\n","\n","Step    420: Ran 10 train steps in 2.24 secs\n","Step    420: train WeightedCategoryCrossEntropy |  0.44711867\n","Step    420: eval  WeightedCategoryCrossEntropy |  0.67765743\n","Step    420: eval      WeightedCategoryAccuracy |  0.62500000\n","\n","Step    430: Ran 10 train steps in 2.23 secs\n","Step    430: train WeightedCategoryCrossEntropy |  0.49320555\n","Step    430: eval  WeightedCategoryCrossEntropy |  0.41588357\n","Step    430: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    440: Ran 10 train steps in 2.21 secs\n","Step    440: train WeightedCategoryCrossEntropy |  0.54100281\n","Step    440: eval  WeightedCategoryCrossEntropy |  0.40154731\n","Step    440: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    450: Ran 10 train steps in 3.08 secs\n","Step    450: train WeightedCategoryCrossEntropy |  0.56740767\n","Step    450: eval  WeightedCategoryCrossEntropy |  0.57083166\n","Step    450: eval      WeightedCategoryAccuracy |  0.68750000\n","\n","Step    460: Ran 10 train steps in 2.26 secs\n","Step    460: train WeightedCategoryCrossEntropy |  0.44857645\n","Step    460: eval  WeightedCategoryCrossEntropy |  0.37410405\n","Step    460: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    470: Ran 10 train steps in 2.28 secs\n","Step    470: train WeightedCategoryCrossEntropy |  0.52001667\n","Step    470: eval  WeightedCategoryCrossEntropy |  0.52049875\n","Step    470: eval      WeightedCategoryAccuracy |  0.81250000\n","\n","Step    480: Ran 10 train steps in 2.28 secs\n","Step    480: train WeightedCategoryCrossEntropy |  0.50995046\n","Step    480: eval  WeightedCategoryCrossEntropy |  0.39162293\n","Step    480: eval      WeightedCategoryAccuracy |  0.75000000\n","\n","Step    490: Ran 10 train steps in 2.31 secs\n","Step    490: train WeightedCategoryCrossEntropy |  0.43880764\n","Step    490: eval  WeightedCategoryCrossEntropy |  0.27591544\n","Step    490: eval      WeightedCategoryAccuracy |  0.87500000\n","\n","Step    500: Ran 10 train steps in 3.13 secs\n","Step    500: train WeightedCategoryCrossEntropy |  0.51517904\n","Step    500: eval  WeightedCategoryCrossEntropy |  0.56281668\n","Step    500: eval      WeightedCategoryAccuracy |  0.81250000\n"]}],"source":["# Do not modify this cell.\n","# Take a look on how the eval_task is inside square brackets and \n","# take that into account for you train_model implementation\n","training_loop = train_model(model, train_task, [eval_task], 500, output_dir_expand)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"6sxkrvl6AXY-"},"outputs":[],"source":["# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: compute_accuracy\n","def compute_accuracy(preds, y, y_weights):\n","    \"\"\"\n","    Input: \n","        preds: a tensor of shape (dim_batch, output_dim) \n","        y: a tensor of shape (dim_batch,) with the true labels\n","        y_weights: a n.ndarray with the a weight for each example\n","    Output: \n","        accuracy: a float between 0-1 \n","        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n","        sum_weights (np.float32): Sum of the weights\n","    \"\"\"\n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","    # Create an array of booleans, \n","    # True if the probability of positive sentiment is greater than\n","    # the probability of negative sentiment\n","    # else False\n","    is_pos = [pred[1]>pred[0] for pred in preds]\n","\n","    # convert the array of booleans into an array of np.int32\n","    is_pos_int = np.array(1*is_pos)\n","    \n","    # compare the array of predictions (as int32) with the target (labels) of type int32\n","    correct = is_pos_int == y\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"V12p33PmAfyH"},"outputs":[],"source":["# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: compute_accuracy\n","def compute_accuracy(preds, y, y_weights):\n","    \"\"\"\n","    Input: \n","        preds: a tensor of shape (dim_batch, output_dim) \n","        y: a tensor of shape (dim_batch,) with the true labels\n","        y_weights: a n.ndarray with the a weight for each example\n","    Output: \n","        accuracy: a float between 0-1 \n","        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n","        sum_weights (np.float32): Sum of the weights\n","    \"\"\"\n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","    # Create an array of booleans, \n","    # True if the probability of positive sentiment is greater than\n","    # the probability of negative sentiment\n","    # else False\n","    is_pos = [pred[1]>pred[0] for pred in preds]\n","\n","    # convert the array of booleans into an array of np.int32\n","    is_pos_int = np.array(1*is_pos)\n","    \n","    # compare the array of predictions (as int32) with the target (labels) of type int32\n","    correct = is_pos_int == y\n","\n","    # Count the sum of the weights.\n","    sum_weights = np.sum(y_weights)\n","    \n","    # convert the array of correct predictions (boolean) into an arrayof np.float32\n","    correct_float = 1.0*correct\n","    \n","    # Multiply each prediction with its corresponding weight.\n","    weighted_correct_float = correct_float*y_weights\n","\n","    # Sum up the weighted correct predictions (of type np.float32), to go in the\n","    # denominator.\n","    weighted_num_correct = np.sum(weighted_correct_float)\n","\n","    # Divide the number of weighted correct predictions by the sum of the\n","    # weights.\n","    accuracy = weighted_num_correct/sum_weights\n","\n","    ### END CODE HERE ###\n","    return accuracy, weighted_num_correct, sum_weights"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1684132593352,"user":{"displayName":"Rohith surya Podugu","userId":"17322199263682336029"},"user_tz":420},"id":"JnCKSdGFAjq8","outputId":"6301718b-a05d-4ea2-945e-4f56891fe769"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model's prediction accuracy on a single training batch is: 70.3125%\n","Weighted number of correct predictions 90.0; weighted number of total observations predicted 128\n"]}],"source":["# test your function\n","tmp_val_generator = val_generator(128, val_pos\n","                    , val_neg, Vocab, loop=True\n","                    , shuffle = False)\n","\n","# get one batch\n","tmp_batch = next(tmp_val_generator)\n","\n","# Position 0 has the model inputs (tweets as tensors)\n","# position 1 has the targets (the actual labels)\n","tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n","\n","# feed the tweet tensors into the model to get a prediction\n","tmp_pred = training_loop.eval_model(tmp_inputs)\n","tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n","\n","print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n","print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: compute_accuracy\n","def confusioin_matrix(preds, y, y_weights):\n","    \"\"\"\n","    Input: \n","        preds: a tensor of shape (dim_batch, output_dim) \n","        y: a tensor of shape (dim_batch,) with the true labels\n","        y_weights: a n.ndarray with the a weight for each example\n","    Output: \n","        accuracy: a float between 0-1 \n","        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n","        sum_weights (np.float32): Sum of the weights\n","    \"\"\"\n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","    # Create an array of booleans, \n","    # True if the probability of positive sentiment is greater than\n","    # the probability of negative sentiment\n","    # else False\n","    is_pos = [pred[1]>pred[0] for pred in preds]\n","\n","    # convert the array of booleans into an array of np.int32\n","    is_pos_int = np.array(1*is_pos)\n","    \n","    # compare the array of predictions (as int32) with the target (labels) of type int32\n","    correct = is_pos_int == y\n","\n","    # Count the sum of the weights.\n","    sum_weights = np.sum(y_weights)\n","    \n","    # convert the array of correct predictions (boolean) into an arrayof np.float32\n","    correct_float = 1.0*correct\n","    \n","    # Multiply each prediction with its corresponding weight.\n","    weighted_correct_float = correct_float*y_weights\n","\n","    # Sum up the weighted correct predictions (of type np.float32), to go in the\n","    # denominator.\n","    weighted_num_correct = np.sum(weighted_correct_float)\n","\n","    # Divide the number of weighted correct predictions by the sum of the\n","    # weights.\n","    accuracy = weighted_num_correct/sum_weights\n","\n","    ### END CODE HERE ###\n","    return accuracy, weighted_num_correct, sum_weights"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model's prediction accuracy on a single training batch is: 62.945491790771484%\n","Weighted number of correct predictions 3603.0; weighted number of total observations predicted 5724\n"]}],"source":["# test your function\n","tmp_val_generator = val_generator(5724, val_pos\n","                    , val_neg, Vocab, loop=True\n","                    , shuffle = False)\n","\n","# get one batch\n","tmp_batch = next(tmp_val_generator)\n","\n","# Position 0 has the model inputs (tweets as tensors)\n","# position 1 has the targets (the actual labels)\n","tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n","\n","# feed the tweet tensors into the model to get a prediction\n","tmp_pred = training_loop.eval_model(tmp_inputs)\n","tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n","\n","print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n","print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN6xYpYqjt3sHGiAgKWLpil","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
